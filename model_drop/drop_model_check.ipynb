{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6d4a056e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "499124cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nhsiao:chingwei1%7e@\n",
    "#! pip install transformers --proxy=http://fetfw.fareastone.com.tw:8080 --trusted-host=pypi.python.org --trusted-host=pypi.org --trusted-host=files.pythonhosted.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca5e5731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install transformers[sentencepiece] --proxy=http://nhsiao:chingwei1%7e@fetfw.fareastone.com.tw:8080 --trusted-host=pypi.python.org --trusted-host=pypi.org --trusted-host=files.pythonhosted.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1e00312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install datasets==1.18.3 --proxy=http://fetfw.fareastone.com.tw:8080 --trusted-host=pypi.python.org --trusted-host=pypi.org --trusted-host=files.pythonhosted.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75359eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install torch --proxy=http://fetfw.fareastone.com.tw:8080 --trusted-host=pypi.python.org --trusted-host=pypi.org --trusted-host=files.pythonhosted.org\n",
    "# pip install torchvision \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a6d3d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, DataCollatorWithPadding\n",
    "\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('uer/roberta-base-finetuned-chinanews-chinese')\n",
    "tokenizer = AutoTokenizer.from_pretrained('../module/roberta-base-finetuned-chinanews-chinese/')\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed804654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ca3e22",
   "metadata": {},
   "source": [
    "### 產生train & val資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1009160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datapath = 'train_v3(drop).csv' \n",
    "\n",
    "# df = pd.read_csv(datapath)\n",
    "# df.rename(columns={'Drop' : 'label','content':'text'},inplace=True)\n",
    " \n",
    "# df['content'] = df['title']+'[sep]'+ df['text']  \n",
    "# df['label'].value_counts()  # 看drop資料數量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5db40107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.loc[:,['label','content']]\n",
    "# df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7aca3674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(112)\n",
    "## 拆分成df_train, df_val, df_test\n",
    "# df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), \n",
    "#                                      [int(.8*len(df)), int(.9*len(df))])\n",
    "# print(len(df_train),len(df_val),len(df_test))\n",
    "\n",
    "\n",
    "# #拆分成df_train, df_val\n",
    "# df_train, df_val = np.split(df.sample(frac=1, random_state=42), \n",
    "#                                      [int(.8*len(df))])\n",
    "# print(len(df_train),len(df_val)) #, len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba64a8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.to_csv('/home/jovyan/at102-group4/model1_drop/train2.csv',index=False)\n",
    "# df_val.to_csv('/home/jovyan/at102-group4/model1_drop/val2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc516d34",
   "metadata": {},
   "source": [
    "### test資料處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd7bb173",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'final_2022-03-14.csv'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 載入test資料\n",
    "import datetime\n",
    "\n",
    "\n",
    "date = str(datetime.date.today())\n",
    "date_today = 'final_'+ date + '.csv'\n",
    "\n",
    "datapath_test_file = \"./../data/\"\n",
    "datapath_test = os.path.join(datapath_test_file,date_today) \n",
    "\n",
    "df_test_all = pd.read_csv(datapath_test)\n",
    "date_today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2d70fab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 製造一份相同格式的測試資料，僅留標題+內文\n",
    " \n",
    "data = df_test_all['標題']+'[sep]'+ df_test_all['內容']  \n",
    "df_test = pd.DataFrame(data=data,columns=['content'])\n",
    "df_test['label'] = None\n",
    "df_test = df_test[['label','content']]\n",
    "df_test.sample(2)\n",
    "\n",
    "# 存檔處理後的test\n",
    "# save_path_file = '/home/jovyan/at102-group4/model1_drop/test/'\n",
    "save_path_file = './../data/test/'\n",
    "\n",
    "save_path = os.path.join(save_path_file,'test_'+date+'.csv')\n",
    "df_test.to_csv(save_path,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a5bf26",
   "metadata": {},
   "source": [
    "### 載入train val test資料集，並經過tokenize，製成可處理之型式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a569d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 此方法適用jupyter notebook, 不適用 docker\n",
    "# data=load_dataset('csv', data_files={'train':['/home/ec2-user/SageMaker/poas/data/test/train2.csv'], \n",
    "#                                      'valid':['/home/ec2-user/SageMaker/poas/data/test/val2.csv'],\n",
    "#                                      'test':[save_path]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b3c42e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b7edbaf4b2d47054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\nhsiao\\.cache\\huggingface\\datasets\\csv\\default-b7edbaf4b2d47054\\0.0.0\\6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 998.88it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00, 333.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\nhsiao\\.cache\\huggingface\\datasets\\csv\\default-b7edbaf4b2d47054\\0.0.0\\6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 166.71it/s]\n"
     ]
    }
   ],
   "source": [
    "data=load_dataset('csv', data_files={\n",
    "                                     'test':[save_path]}) \n",
    "                                     #'train':['./../data/test/train2.csv'], \n",
    "                                    #  'valid':['./../data/test/val2.csv'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "99ac8226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "36606489",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 28.11ba/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(data:dict):\n",
    "    return tokenizer(data['content'],padding=True,truncation=True, max_length=512)\n",
    "\n",
    "tokenized_data=data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "341ba101",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenized_data.remove_columns(['content'])\n",
    "tokenized_data = tokenized_data.rename_column('label','labels')\n",
    "tokenized_data.set_format('torch',device='cpu')\n",
    "tokenized_data['test'].column_names\n",
    "tokenized_data.set_format('torch',device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "277ebb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCHSIZE = 8\n",
    "\n",
    "# train_dataloader = DataLoader(\n",
    "#     tokenized_data[\"train\"], shuffle=True, batch_size=BATCHSIZE, collate_fn=data_collator\n",
    "# )\n",
    "# valid_dataloader = DataLoader(\n",
    "#     tokenized_data[\"valid\"], shuffle=False, batch_size=BATCHSIZE, collate_fn=data_collator\n",
    "# )\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    tokenized_data[\"test\"], shuffle=False, batch_size=BATCHSIZE, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2d506651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in train_dataloader:\n",
    "#     break\n",
    "# {k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c87989",
   "metadata": {},
   "source": [
    "### RoBERTa_model定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59db01e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setting up the device for GPU usage\n",
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "45fe2a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): RoBERTaClass(\n",
       "    (l1): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(21128, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (2): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (3): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (4): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (5): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (6): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (7): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (8): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (9): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (10): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (11): BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): BertPooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (l2): Dropout(p=0.3, inplace=False)\n",
       "    (l3): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RoBERTa_model = AutoModelForSequenceClassification.from_pretrained('../module/roberta-base-finetuned-chinanews-chinese')\n",
    "# for param in RoBERTa_model.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "class RoBERTaClass(torch.nn.Module):\n",
    "    def __init__(self):  # 建造layer積木\n",
    "        super(RoBERTaClass, self).__init__()\n",
    "        self.l1 = RoBERTa_model.base_model\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 2)  # dense layer類別數量:2\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):  # 組裝積木\n",
    "        pooler_output = self.l1(input_ids, attention_mask, token_type_ids)[1]\n",
    "        output_2 = self.l2(pooler_output)\n",
    "        output = self.l3(output_2)\n",
    "        return output\n",
    "    \n",
    "model = RoBERTaClass()\n",
    "model.load_state_dict(torch.load('./save_test_v2.pth', map_location=torch.device('cpu'))) \n",
    "model = torch.nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bbb89186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function定義\n",
    "\n",
    "LEARNING_RATE = 3e-05\n",
    "\n",
    "def loss_fn(output, labels):\n",
    "    return torch.nn.CrossEntropyLoss()(output, labels)\n",
    "\n",
    "optimizer = torch.optim.AdamW(params =  model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "79e95987",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "\n",
    "def train(epoch):\n",
    "    model.train() #將 model 設為 training mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    for data in tqdm(train_dataloader):\n",
    "        input_ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "        attention_mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "        labels = data['labels'].to(device, dtype = torch.long)\n",
    "        \n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    \n",
    "    train_loss = total_loss/len(train_dataloader)\n",
    "    train_losses.append(train_loss)\n",
    "    print(f'Epoch:{epoch+1}, Trianing Loss:{total_loss}')\n",
    "    \n",
    "    # return train_loss 比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ed486434",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_losses = []\n",
    "eval_accu = []\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "def evaluation(eval_data):\n",
    "    model.eval() #將 model 設為 evaluation mode\n",
    "    total_loss = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    \n",
    "    print('Evaluating...')\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(eval_data):\n",
    "            input_ids = data['input_ids'].to(device, dtype = torch.long)\n",
    "            attention_mask = data['attention_mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            labels = data['labels'].to(device, dtype = torch.long)\n",
    "\n",
    "            print(data)\n",
    "            print(len(input_ids))\n",
    "            print(token_type_ids)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "            _, predict = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predict.eq(labels).sum().item()\n",
    "            # loss = loss_fn(outputs, labels)\n",
    "            # total_loss += loss.item()\n",
    "            \n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predict.cpu().numpy())\n",
    "            \n",
    "    # accu = 100.*correct/total\n",
    "    # eval_loss = total_loss/len(eval_data)\n",
    "    # eval_losses.append(eval_loss)\n",
    "    # eval_accu.append(accu)\n",
    "    # print(f'Evaluation Loss:{total_loss}, Accuracy:{accu:.3f}%')\n",
    "    \n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37112675",
   "metadata": {},
   "source": [
    "### 訓練及預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "680bcc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9ccd049869457e91884b9814982bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'labels': tensor([nan, nan, nan, nan, nan, nan, nan, nan]), 'input_ids': tensor([[  101,   138,  6554,  ...,  9059, 10782,   102],\n",
      "        [  101,   138,  6554,  ...,     0,     0,     0],\n",
      "        [  101,   138,  6554,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,   138,  6554,  ...,     0,     0,     0],\n",
      "        [  101,   138,  2658,  ...,  8148,  8175,   102],\n",
      "        [  101,   138,  2658,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "8\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "{'labels': tensor([nan, nan, nan, nan, nan, nan, nan, nan]), 'input_ids': tensor([[ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        [ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        [ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        [ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        [ 101,  138, 2658,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "8\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "{'labels': tensor([nan, nan, nan, nan, nan, nan, nan, nan]), 'input_ids': tensor([[ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        [ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        [ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        [ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        [ 101,  138, 2658,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "8\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "{'labels': tensor([nan, nan, nan, nan, nan, nan, nan, nan]), 'input_ids': tensor([[ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        [ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        [ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        [ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        [ 101,  138, 2658,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "8\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "{'labels': tensor([nan, nan, nan, nan, nan, nan, nan, nan]), 'input_ids': tensor([[ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        [ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        [ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        [ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        [ 101,  138, 2658,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "8\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "{'labels': tensor([nan, nan, nan, nan, nan, nan, nan, nan]), 'input_ids': tensor([[ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        [ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        [ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101,  138, 2658,  ...,    0,    0,    0],\n",
      "        [ 101, 2695, 6525,  ..., 1243, 8024,  102],\n",
      "        [ 101, 2695, 6525,  ..., 1243, 8024,  102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "8\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "{'labels': tensor([nan, nan, nan, nan]), 'input_ids': tensor([[ 101, 7427, 7770,  ..., 7442,  510,  102],\n",
      "        [ 101, 4373, 2255,  ...,    0,    0,    0],\n",
      "        [ 101, 7427, 7770,  ..., 7442,  510,  102],\n",
      "        [ 101, 4373, 2255,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "4\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# 預測\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_pred=evaluation(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "83d1e1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 整理結果\n",
    "my_predict = df_test_all.join(pd.DataFrame(y_pred))\n",
    "my_predict.rename(columns={0 : 'drop'},inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "21bb3d1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>標題</th>\n",
       "      <th>推回文類別</th>\n",
       "      <th>內容</th>\n",
       "      <th>發文時間</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[購機] USD500? 中華及AT&amp;T均支援VoLTE+VoWifi</td>\n",
       "      <td>推文</td>\n",
       "      <td>有Fi的話 Google Voice的號碼等於你Fi的號碼 在美國用SIM卡打 在其他地方用...</td>\n",
       "      <td>03/13 16:14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[情報] 亞太電信，低價方案【只到今天】。</td>\n",
       "      <td>推文</td>\n",
       "      <td>風水寶地真的賺爛</td>\n",
       "      <td>03/13 18:35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[情報] 亞太電信，低價方案【只到今天】。</td>\n",
       "      <td>推文</td>\n",
       "      <td>這種好像就不是終身？</td>\n",
       "      <td>03/13 19:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[情報] 亞太電信，低價方案【只到今天】。</td>\n",
       "      <td>推文</td>\n",
       "      <td>按以往截止日應該會繼續後延吧。</td>\n",
       "      <td>03/13 19:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[情報] 亞太電信，低價方案【只到今天】。</td>\n",
       "      <td>推文</td>\n",
       "      <td>截止日從3/31 → 3/13， 遠傳已經出手， 再不把握沒機會</td>\n",
       "      <td>03/13 20:09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[情報] 亞太電信，低價方案【只到今天】。</td>\n",
       "      <td>推文</td>\n",
       "      <td>這次是原本3/1說會延到3/31，但3/5突然說只到3/13截止。https://i.img...</td>\n",
       "      <td>03/13 20:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[情報] 亞太電信，低價方案【只到今天】。</td>\n",
       "      <td>推文</td>\n",
       "      <td>幫家父升級88https://i.imgur.com/IX3ZnO8.jpg</td>\n",
       "      <td>03/13 22:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[情報] 亞太電信，低價方案【只到今天】。</td>\n",
       "      <td>推文</td>\n",
       "      <td>卡片可以去萊爾富買，還可多一張發票。</td>\n",
       "      <td>03/13 21:34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[情報] 亞太電信，低價方案【只到今天】。</td>\n",
       "      <td>推文</td>\n",
       "      <td>免綁約想問假如要停掉要違約金嗎</td>\n",
       "      <td>03/13 21:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[情報] 亞太電信，低價方案【只到今天】。</td>\n",
       "      <td>推文</td>\n",
       "      <td>http://i.imgur.com/JAXEKc6.jpg 這裏</td>\n",
       "      <td>03/13 21:37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[情報] 亞太電信，低價方案【只到今天】。</td>\n",
       "      <td>推文</td>\n",
       "      <td>有綁約才會違約</td>\n",
       "      <td>03/13 21:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[情報] 亞太電信，低價方案【只到今天】。</td>\n",
       "      <td>推文</td>\n",
       "      <td>獲利王新聞稿照單全收 還沒到合併日就卡卡的動滋，這是為什麼？！[方案] 台星188/88暫代...</td>\n",
       "      <td>03/13 22:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[情報] 亞太電信，低價方案【只到今天】。</td>\n",
       "      <td>推文</td>\n",
       "      <td>請問如果不打算開通的話不取貨就會退款嗎謝謝</td>\n",
       "      <td>03/13 23:32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[情報] 亞太電信，低價方案【只到今天】。</td>\n",
       "      <td>推文</td>\n",
       "      <td>續約188有終身嗎 好像沒看到資訊</td>\n",
       "      <td>03/14 00:22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     標題 推回文類別  \\\n",
       "4   [購機] USD500? 中華及AT&T均支援VoLTE+VoWifi    推文   \n",
       "10                [情報] 亞太電信，低價方案【只到今天】。    推文   \n",
       "13                [情報] 亞太電信，低價方案【只到今天】。    推文   \n",
       "18                [情報] 亞太電信，低價方案【只到今天】。    推文   \n",
       "22                [情報] 亞太電信，低價方案【只到今天】。    推文   \n",
       "24                [情報] 亞太電信，低價方案【只到今天】。    推文   \n",
       "27                [情報] 亞太電信，低價方案【只到今天】。    推文   \n",
       "28                [情報] 亞太電信，低價方案【只到今天】。    推文   \n",
       "29                [情報] 亞太電信，低價方案【只到今天】。    推文   \n",
       "30                [情報] 亞太電信，低價方案【只到今天】。    推文   \n",
       "31                [情報] 亞太電信，低價方案【只到今天】。    推文   \n",
       "37                [情報] 亞太電信，低價方案【只到今天】。    推文   \n",
       "40                [情報] 亞太電信，低價方案【只到今天】。    推文   \n",
       "41                [情報] 亞太電信，低價方案【只到今天】。    推文   \n",
       "\n",
       "                                                   內容          發文時間  \n",
       "4   有Fi的話 Google Voice的號碼等於你Fi的號碼 在美國用SIM卡打 在其他地方用...   03/13 16:14  \n",
       "10                                           風水寶地真的賺爛   03/13 18:35  \n",
       "13                                         這種好像就不是終身？   03/13 19:02  \n",
       "18                                    按以往截止日應該會繼續後延吧。   03/13 19:34  \n",
       "22                   截止日從3/31 → 3/13， 遠傳已經出手， 再不把握沒機會   03/13 20:09  \n",
       "24  這次是原本3/1說會延到3/31，但3/5突然說只到3/13截止。https://i.img...   03/13 20:34  \n",
       "27             幫家父升級88https://i.imgur.com/IX3ZnO8.jpg   03/13 22:37  \n",
       "28                                 卡片可以去萊爾富買，還可多一張發票。   03/13 21:34  \n",
       "29                                    免綁約想問假如要停掉要違約金嗎   03/13 21:36  \n",
       "30                  http://i.imgur.com/JAXEKc6.jpg 這裏   03/13 21:37  \n",
       "31                                            有綁約才會違約   03/13 21:43  \n",
       "37  獲利王新聞稿照單全收 還沒到合併日就卡卡的動滋，這是為什麼？！[方案] 台星188/88暫代...   03/13 22:28  \n",
       "40                              請問如果不打算開通的話不取貨就會退款嗎謝謝   03/13 23:32  \n",
       "41                                  續約188有終身嗎 好像沒看到資訊   03/14 00:22  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop之內容\n",
    "df_drop = my_predict.loc[my_predict['drop']==1].loc[:,'標題':'發文時間']\n",
    "\n",
    "# 紀錄過濾掉的資料\n",
    "drop_final_save_path_file = './../data/no_use_data/'\n",
    "drop_final_save_path = os.path.join(drop_final_save_path_file,'data_'+date+'.csv')\n",
    "df_drop.to_csv(drop_final_save_path,index=False)\n",
    "\n",
    "df_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4c4107f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nhsiao\\AppData\\Local\\Temp\\ipykernel_22736\\736907754.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result.drop(columns='drop',inplace=True)\n",
      "C:\\Users\\nhsiao\\AppData\\Local\\Temp\\ipykernel_22736\\736907754.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result['crawel_type'] = crawel_type\n",
      "C:\\Users\\nhsiao\\AppData\\Local\\Temp\\ipykernel_22736\\736907754.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  result.drop(columns='filename',inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# 留下非drop之資料並輸出\n",
    "result = my_predict.loc[my_predict['drop']==0]\n",
    "result.drop(columns='drop',inplace=True)\n",
    "\n",
    "\n",
    "# 列出crawel_type                                   \n",
    "crawel_type = [f.split(\"_\")[0] for f in result[\"filename\"]]\n",
    "crawel_type\n",
    "\n",
    "result['crawel_type'] = crawel_type\n",
    "result.drop(columns='filename',inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "08485e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_save_path_file = './../data/'\n",
    "final_save_path = os.path.join(final_save_path_file,'final_'+date+'.csv')\n",
    "result.to_csv(final_save_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "407d41f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------save ok------------------------\n",
      "Save path:./../data/final_2022-03-14.csv\n",
      "Original data : 52, Keep data: 38, Drop data:14\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------save ok------------------------')\n",
    "print(f'Save path:{final_save_path}')\n",
    "print(f'Original data : {len(my_predict)}, Keep data: {len(result)}, Drop data:{len(df_drop)}')\n",
    "print('-------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3a6e9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ab4e0fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to convert all *.ipynb files from current directory to python script, you can run the command like this:\n",
    "#!pip install ipython\n",
    "#!pip install nbconvert\n",
    "#!jupyter nbconvert --to script *.ipynb(轉換成 *.py)\n",
    "\n",
    "# 只要執行一次, 更新*.py,即可remark, \n",
    "# 若要用 docker, 要將 *.py中的 pip install transformer 註解, docker有安裝\n",
    "#!jupyter nbconvert --to script drop_model.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6f4de7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93142bfb35f5afb420ab1de6cb1e3c843ba21cfc0819b7ae0900097ae81dc343"
  },
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
